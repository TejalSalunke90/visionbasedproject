{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-bpAptnGBf4A"
   },
   "source": [
    "##Acne detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import csv\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eYIOMzGIU8hK"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## pip install split-folders\n",
    "import splitfolders \n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EwH4PW0EU8hL"
   },
   "outputs": [],
   "source": [
    "input1=r\"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\dataset\\acne infantile\"\n",
    "input2=r\"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\dataset\\acne vulgaris\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "doV6ab0MBokM"
   },
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kJgX6uYT1r8V",
    "outputId": "ff00ee53-e31f-4706-d863-95d4f1330c13"
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "for filename in os.listdir(input1):\n",
    "   \n",
    "    path=os.path.join(input1,filename)\n",
    "    a=cv2.imread(path)\n",
    "    path\n",
    "    #resize image\n",
    "    resize=(280,430)\n",
    "    img=cv2.resize(a,resize)\n",
    "    \n",
    "    #gray image\n",
    "    path= r\"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\dataset\\acne infantile\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imwrite(os.path.join(path, str(i)+'.jpg'), gray)\n",
    "    i=i+1\n",
    "\n",
    "    cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for vulgaris dataset\n",
    "i=0\n",
    "for filename in os.listdir(input2):\n",
    "   \n",
    "    path=os.path.join(input2,filename)\n",
    "    a=cv2.imread(path)\n",
    "    path\n",
    "    #resize image\n",
    "    resize=(280,430)\n",
    "    img=cv2.resize(a,resize)\n",
    "    \n",
    "    #gray image\n",
    "    path= r\"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\dataset\\acne vulgaris\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imwrite(os.path.join(path, str(i)+'.jpg'), gray)\n",
    "    i=i+1\n",
    "\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\tejal\\\\Desktop\\\\Tejal Salunke\\\\college\\\\CV\\\\CV project\\\\dataset\\\\acne vulgaris'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for dirname in os.listdir(\".\"):\\n    if os.path.isdir(dirname):\\n        for i, filename in enumerate(os.listdir(dirname)):\\n            os.rename(dirname + \"/\" + filename, dirname + \"/\" + str(i) + \".bmp\")'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for dirname in os.listdir(\".\"):\n",
    "    if os.path.isdir(dirname):\n",
    "        for i, filename in enumerate(os.listdir(dirname)):\n",
    "            os.rename(dirname + \"/\" + filename, dirname + \"/\" + str(i) + \".bmp\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename all the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'os.getcwd()  #  Location of the current working directory\\npath = r\"C:/Users/tejal/Desktop/Tejal Salunke/college/CV/CV project/dataset/7/\"\\ncollection = path\\nfor i, filename in enumerate(os.listdir(collection)):\\n    os.rename(path+ filename, path + \"acne infantile pre\" +str(i) + \".jpg\")'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''os.getcwd()  #  Location of the current working directory\n",
    "path = r\"C:/Users/tejal/Desktop/Tejal Salunke/college/CV/CV project/dataset/7/\"\n",
    "collection = path\n",
    "for i, filename in enumerate(os.listdir(collection)):\n",
    "    os.rename(path+ filename, path + \"acne infantile pre\" +str(i) + \".jpg\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'os.getcwd()  #  Location of the current working directory\\n\\npath = \"C:/Users/jyoti/JRMWork/cvlab/CP_Vehicle_class/data/Bus/\"\\ncollection = path\\nfor i, filename in enumerate(os.listdir(collection)):\\n    os.rename(path+ filename, path + \"bus\"+ str(i) + \".jpg\")  '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''os.getcwd()  #  Location of the current working directory\n",
    "\n",
    "path = \"C:/Users/jyoti/JRMWork/cvlab/CP_Vehicle_class/data/Bus/\"\n",
    "collection = path\n",
    "for i, filename in enumerate(os.listdir(collection)):\n",
    "    os.rename(path+ filename, path + \"bus\"+ str(i) + \".jpg\")  '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#The above cells need to be executed only once\n",
    "#So comment after renaming\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 18980 files [01:05, 288.89 files/s]\n"
     ]
    }
   ],
   "source": [
    "input_folder = r\"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\dataset\"\n",
    "output = r\"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\dataset\" \n",
    "\n",
    "# .75, .25 \n",
    "\n",
    "splitfolders.ratio(input_folder, output, seed=10, ratio=(.8, .2))\n",
    "\n",
    "# splitfolders.ratio(input_folder, output=\"cell_images2\", seed=42, ratio=(.75, .25)) \n",
    "# splitfolders.ratio(input_folder, output=\"cell_images2\", seed=42, ratio=(.7, .2, .1))\n",
    "# .8  .1 .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = r\"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\dataset\"\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_infantile_dir = os.path.join(train_dir, 'acne infantile')  # directory with our training cat pictures\n",
    "train_vulgaris_dir = os.path.join(train_dir, 'acne vulgaris')  # directory with our training dog pictures\n",
    "validation_infantile_dir = os.path.join(validation_dir, 'acne infantile')  # directory with our validation cat pictures\n",
    "validation_vulgaris_dir = os.path.join(validation_dir, 'acne vulgaris')  # directory with our validation dog pictures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the data folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_infantile_tr = len(os.listdir(train_infantile_dir))\n",
    "num_vulgaris_tr = len(os.listdir(train_vulgaris_dir))\n",
    "\n",
    "num_infantile_val = len(os.listdir(validation_infantile_dir))\n",
    "num_vulgaris_val = len(os.listdir(validation_vulgaris_dir))\n",
    "\n",
    "total_train = num_infantile_tr + num_vulgaris_tr\n",
    "total_val = num_infantile_val + num_vulgaris_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  11338 val:  4184\n"
     ]
    }
   ],
   "source": [
    "print(\"train: \", total_train, \"val: \", total_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    zoom_range=0.15, # .1, .2, .3\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    #rescale=1./255,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 =  r\"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\dataset\\train\\pre acne infantile\"\n",
    "input2 =  r\"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\dataset\\train\\pre acne vulgaris\"\n",
    "\n",
    "i=0\n",
    "for filename in os.listdir(input1):\n",
    "   \n",
    "    path=os.path.join(input1,filename)\n",
    "    pic = load_img(path)\n",
    "    pic.getpixel\n",
    "    pic_array = img_to_array(pic)\n",
    "    pic_array = pic_array.reshape((1,) + pic_array.shape)\n",
    "\n",
    "    count = 0\n",
    "    output = r\"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\dataset\\train\\acne infantile\"\n",
    "    for batch in datagen.flow( pic_array, batch_size = 1, save_to_dir=output,  save_prefix='infantile', save_format='jpg'):\n",
    "        count += 1\n",
    "        if count > 10:\n",
    "            break\n",
    "\n",
    "i=0\n",
    "for filename in os.listdir(input2):\n",
    "   \n",
    "    path=os.path.join(input2,filename)\n",
    "    pic = load_img(path)\n",
    "    pic.getpixel\n",
    "    pic_array = img_to_array(pic)\n",
    "    pic_array = pic_array.reshape((1,) + pic_array.shape)\n",
    "\n",
    "    count = 0\n",
    "    output = r\"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\dataset\\train\\acne vulgaris\"\n",
    "    for batch in datagen.flow( pic_array, batch_size = 1, save_to_dir=output,  save_prefix='vulgaris', save_format='jpg'):\n",
    "        count += 1\n",
    "        if count > 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 248-249: truncated \\UXXXXXXXX escape (Temp/ipykernel_15984/4219323456.py, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\tejal\\AppData\\Local\\Temp/ipykernel_15984/4219323456.py\"\u001b[1;36m, line \u001b[1;32m15\u001b[0m\n\u001b[1;33m    break'''\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 248-249: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''i=0\n",
    "for filename in os.listdir(input2):\n",
    "   \n",
    "    path=os.path.join(input2,filename)\n",
    "    pic = load_img(path)\n",
    "    pic.getpixel\n",
    "    pic_array = img_to_array(pic)\n",
    "    pic_array = pic_array.reshape((1,) + pic_array.shape)\n",
    "\n",
    "    count = 0\n",
    "    output = \"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\dataset\\train\\acne infantile\"\n",
    "    for batch in datagen.flow( pic_array, batch_size = 1, save_to_dir=output,  save_prefix='auto', save_format='jpg'):\n",
    "        count += 1\n",
    "        if count > 10:\n",
    "            break'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''input1=  r\"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\dataset\\train\\acne infantile\"\n",
    "input2=  r\"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\dataset\\train\\acne vulgaris\"\n",
    "# cv2.getGaborKernel(ksize, sigma, theta, lambda, gamma, psi, ktype)\n",
    "# ksize - size of gabor filter (n, n)\n",
    "# sigma - standard deviation of the gaussian function\n",
    "# theta - orientation of the normal to the parallel stripes\n",
    "# lambda - wavelength of the sunusoidal factor\n",
    "# gamma - spatial aspect ratio\n",
    "# psi - phase offset\n",
    "# ktype - type and range of values that each pixel in the gabor kernel can hold\n",
    "\n",
    "\n",
    "g_kernel = cv2.getGaborKernel((21, 21), 8.0, np.pi/4, 10.0, 0.5, 0, ktype=cv2.CV_32F)\n",
    "\n",
    "\n",
    "img = cv2.imread(r\"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\dataset\\acne infantile\\0.jpg\")\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "filtered_img = cv2.filter2D(img, cv2.CV_8UC3, g_kernel)\n",
    "\n",
    "cv2.imshow('image', img)\n",
    "cv2.imshow('filtered image', filtered_img)\n",
    "\n",
    "\n",
    "h, w = g_kernel.shape[:2]\n",
    "g_kernel = cv2.resize(g_kernel, (3*w, 3*h), interpolation=cv2.INTER_CUBIC)\n",
    "cv2.imshow('gabor kernel (resized)', g_kernel)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UqtvEHzs1r8Y",
    "outputId": "5221f802-20b3-480b-d36c-f8f2f9525b22"
   },
   "outputs": [],
   "source": [
    "'''input1=  r\"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\dataset\\train\\acne infantile\"\n",
    "input2=  r\"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\dataset\\train\\acne vulgaris\"\n",
    "# cv2.getGaborKernel(ksize, sigma, theta, lambda, gamma, psi, ktype)\n",
    "# ksize - size of gabor filter (n, n)\n",
    "# sigma - standard deviation of the gaussian function\n",
    "# theta - orientation of the normal to the parallel stripes\n",
    "# lambda - wavelength of the sunusoidal factor\n",
    "# gamma - spatial aspect ratio\n",
    "# psi - phase offset\n",
    "# ktype - type and range of values that each pixel in the gabor kernel can hold\n",
    "\n",
    "i=0\n",
    "for filename in os.listdir(input1):\n",
    "    \n",
    "    path=os.path.join(input1,filename)\n",
    "    img=cv2.imread(path)\n",
    "g_kernel = cv2.getGaborKernel((21, 21), 8.0, np.pi/4, 10.0, 0.5, 0, ktype=cv2.CV_32F)\n",
    "\n",
    "\n",
    "\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "filtered_img = cv2.filter2D(img, cv2.CV_8UC3, g_kernel)\n",
    "ret, labels = cv2.connectedComponents(img)\n",
    "label_hue = np.uint8(179*labels/np.max(labels))\n",
    "blank_ch = 255*np.ones_like(label_hue)\n",
    "labeled_img = cv2.merge([label_hue, blank_ch, blank_ch])\n",
    "labeled_img = cv2.cvtColor(labeled_img, cv2.COLOR_HSV2BGR)\n",
    "labeled_img[label_hue==0] = 0\n",
    "cv2.imshow('labeled.png', labeled_img)\n",
    "cv2.waitKey()\n",
    "\n",
    "def build_filters():\n",
    "    filters = []\n",
    "    ksize = 31\n",
    "    for theta in np.arange(0, np.pi, np.pi / 16):\n",
    "        kern = cv2.getGaborKernel((ksize, ksize), 4.0, theta, 10.0, 0.5, 0, \n",
    "                  ktype=cv2.CV_32F)\n",
    "        kern /= 1.5*kern.sum()\n",
    "        filters.append(kern)\n",
    "        return filters\n",
    "\n",
    "def process(img, filters):\n",
    "    accum = np.zeros_like(img)\n",
    "    for kern in filters:\n",
    "        fimg = cv2.filter2D(img, cv2.CV_8UC3, kern)\n",
    "        np.maximum(accum, fimg, accum)\n",
    "        return accum\n",
    "\n",
    "filters=build_filters()\n",
    "res1=process(img,filters)\n",
    "cv2.imshow('result',res1)\n",
    "img = cv2.imread('yourfile.jpg')\n",
    "\n",
    "def save_my_img(frame_count, img):\n",
    "    path = 'C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\Gk'\n",
    "    name = f'/img_title_{frame_count}.jpg'\n",
    "    cv2.imwrite(os.path.join(path, name), img)\n",
    "\n",
    "# give it a call\n",
    "save_my_img(1, img)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import glob\n",
    "\n",
    "img_dir = r\"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\dataset\\train\\acne infantile\"\n",
    "data_path = os.path.join(img_dir,'*g')\n",
    "files = glob.glob(data_path)\n",
    "data = []\n",
    "for f1 in files:\n",
    "    img = cv2.imread(f1,0)\n",
    "    data.append(img)\n",
    "    img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)[1]  \n",
    "    ret, labels = cv2.connectedComponents(img)\n",
    "    label_hue = np.uint8(179*labels/np.max(labels))\n",
    "    blank_ch = 255*np.ones_like(label_hue)\n",
    "    labeled_img = cv2.merge([label_hue, blank_ch, blank_ch])\n",
    "    labeled_img = cv2.cvtColor(labeled_img, cv2.COLOR_HSV2BGR)\n",
    "    labeled_img[label_hue==0] = 0\n",
    "    cv2.imshow('labeled.png', labeled_img)\n",
    "    cv2.waitKey()\n",
    "\n",
    "    def build_filters():\n",
    "        filters = []\n",
    "        ksize = 31\n",
    "        for theta in np.arange(0, np.pi, np.pi / 16):\n",
    "            kern = cv2.getGaborKernel((ksize, ksize), 4.0, theta, 10.0, 0.5, 0, ktype=cv2.CV_32F)\n",
    "            kern /= 1.5*kern.sum()\n",
    "            filters.append(kern)\n",
    "            return filters\n",
    "\n",
    "    def process(img, filters):\n",
    "        accum = np.zeros_like(img)\n",
    "        for kern in filters:\n",
    "            fimg = cv2.filter2D(img, cv2.CV_8UC3, kern)\n",
    "            np.maximum(accum, fimg, accum)\n",
    "            return accum\n",
    "\n",
    "    filters=build_filters()\n",
    "    res1=process(img,filters)\n",
    "    cv2.imshow('result',res1)\n",
    "    cv2.waitKey(0)\n",
    "    #cv2.destroyAllWindows()'''\n",
    "    cv2.imwrite(\"checking.tif\",res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vekMqwOwI4Iz",
    "outputId": "b0e37953-cb78-47fd-ed88-a5735a10d125"
   },
   "outputs": [],
   "source": [
    "#feature vector dimensions\n",
    "#bus = 1048576 X 128 = 3717199282 x 128\n",
    "#auto = 1048576 x 128 = 134217728 x 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Median filter , Gaussian, bilateral\n",
    "\n",
    "'''input1=  r\"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\dataset\\train\\acne infantile\"\n",
    "input2=  r\"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\dataset\\train\\acne vulgaris\"\n",
    "\n",
    "ksize = (3, 3)  # Box filter for blurring\n",
    "i=0\n",
    "for filename in os.listdir(input1):\n",
    "    \n",
    "    path=os.path.join(input1,filename)\n",
    "    imgblr=cv2.imread(path)\n",
    "    imgGausblr = cv2.GaussianBlur(imgblr, ksize, cv2.BORDER_DEFAULT) \n",
    "    \n",
    "    #initialise sift descriptor\n",
    "    #sift = cv2.xfeatures2d.SIFT_create()\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(imgGausblr, None)\n",
    "    \n",
    "    #convert the descriptor array into a dataframe format\n",
    "    out=pd.DataFrame(descriptors)\n",
    "    #print(\"descriptor shape \",i,\" : \", out.shape)\n",
    "    i=i+1\n",
    "    #drop first coloumn as it's the no of feature detected. Not required.append to the csv file\n",
    "    csv_data=out.to_csv('infantile_blur.csv', mode='a', header=False,index=False)\n",
    "\n",
    "    # find and draw the keypoints\n",
    "\n",
    "    print( \"Total Keypoints wit blur: {}\".format(len(keypoints)) )'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mahotas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15436/3474952086.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmahotas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mahotas'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from sklearn.svm import LinearSVC\n",
    "import mahotas as mt\n",
    "\n",
    "\n",
    "img_dir = r\"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\dataset\\train\\acne infantile\"\n",
    "data_path = os.path.join(img_dir,'*g')\n",
    "files = glob.glob(data_path)\n",
    "data = []\n",
    "num=0\n",
    "for f1 in files:\n",
    "    img = cv2.imread(f1,0)\n",
    "    data.append(img)\n",
    "    img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)[1]  \n",
    "    ret, labels = cv2.connectedComponents(img)\n",
    "    label_hue = np.uint8(179*labels/np.max(labels))\n",
    "    blank_ch = 255*np.ones_like(label_hue)\n",
    "    labeled_img = cv2.merge([label_hue, blank_ch, blank_ch])\n",
    "    labeled_img = cv2.cvtColor(labeled_img, cv2.COLOR_HSV2BGR)\n",
    "    labeled_img[label_hue==0] = 0\n",
    "    cv2.imshow('labeled.png', labeled_img)\n",
    "    cv2.waitKey()\n",
    "    def build_filters():\n",
    "        filters = []\n",
    "        ksize = 31\n",
    "        for theta in np.arange(0, np.pi, np.pi / 16):\n",
    "            kern = cv2.getGaborKernel((ksize, ksize), 4.0, theta, 10.0, 0.5, 0, ktype=cv2.CV_32F)\n",
    "            kern /= 1.5*kern.sum()\n",
    "            filters.append(kern)\n",
    "            return filters\n",
    "    def process(img, filters):\n",
    "        accum = np.zeros_like(img)\n",
    "        for kern in filters:\n",
    "            fimg = cv2.filter2D(img, cv2.CV_8UC3, kern)\n",
    "            np.maximum(accum, fimg, accum)\n",
    "            return accum    \n",
    "    filters=build_filters()\n",
    "    res1=process(img,filters)\n",
    "    cv2.imshow('result',res1)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    path = r\"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\dataset\\train\\acne infantile\"\n",
    "    path=os.path.normpath(path)\n",
    "    path=os.path.join(path,'pic'+str(num)+'.png')\n",
    "    print (path)\n",
    "    cv2.imwrite(path,res1)\n",
    "    num=num+1\n",
    "\n",
    "def extract_features(image):\n",
    "    # calculate haralick texture features for 4 types of adjacency\n",
    "    textures = mt.features.haralick(image)\n",
    "\n",
    "    # take the mean of it and return it\n",
    "    ht_mean  = textures.mean(axis=0)\n",
    "    return ht_mean\n",
    "\n",
    "train_path  = r\"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\dataset\\train\"\n",
    "train_names = os.listdir(train_path)\n",
    "\n",
    "train_features = [[]]\n",
    "train_labels   = [[]]\n",
    "i = 1\n",
    "print (\"[STATUS] Started extracting haralick textures..\")\n",
    "for train_name in train_names:\n",
    "    cur_path = train_path + \"/\" + train_name\n",
    "    cur_label = train_name\n",
    "    i = 1\n",
    "\n",
    "    for file in glob.glob(cur_path + \"/*.png\"):\n",
    "        print (\"Processing Image - {} in {}\".format(i, cur_label))\n",
    "        # read the training image\n",
    "        image = cv2.imread(file)\n",
    "\n",
    "        # convert the image to grayscale\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # extract haralick texture from the image\n",
    "        features = extract_features(gray)\n",
    "\n",
    "        # append the feature vector and label\n",
    "        train_features.append(features)\n",
    "        train_labels.append(cur_label)\n",
    "\n",
    "        # show loop update\n",
    "        i += 1\n",
    "\n",
    "# have a look at the size of our feature vector and labels\n",
    "print (\"Training features: {}\".format(np.array(train_features).shape))\n",
    "print (\"Training labels: {}\".format(np.array(train_labels).shape))\n",
    "\n",
    "# create the classifier\n",
    "print (\"[STATUS] Creating the classifier..\")\n",
    "clf_svm = LinearSVC(random_state=9)\n",
    "\n",
    "# fit the training data and labels\n",
    "print (\"[STATUS] Fitting data/label to model..\")\n",
    "clf_svm.fit(train_features, train_labels)\n",
    "\n",
    "# loop over the test images\n",
    "test_path = r\"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\dataset\\val\"\n",
    "for file in glob.glob(test_path + \"/*.png\"):\n",
    "    # read the input image\n",
    "    image = cv2.imread(file)\n",
    "\n",
    "    # convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # extract haralick texture from the image\n",
    "    features = extract_features(gray)\n",
    "\n",
    "    # evaluate the model and predict label\n",
    "    prediction = clf_svm.predict(features.reshape(1, -1))[0]\n",
    "\n",
    "    # show the label\n",
    "    cv2.putText(image, prediction, (20,30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0,255,255), 3)\n",
    "    print (\"Prediction - {}\".format(prediction))\n",
    "\n",
    "    # display the output image\n",
    "    cv2.imshow(\"Test_Image\", image)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ES6Y7PTsBxrN",
    "outputId": "0f6911ce-99fc-422f-efb9-a73d3fdcb0a3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15436/3028432171.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'acne_infantile.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'uint8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "data1 = pd.read_csv(r'acne_infantile.csv',header=None,dtype='uint8')\n",
    "data1 = data1.astype(np.uint8)\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LFVFo3YO1r8Z",
    "outputId": "07da2e8d-8c31-42a2-a5bb-6abc2c4c5ac6"
   },
   "outputs": [],
   "source": [
    "data2= pd.read_csv(r'acne_vulgaris.csv',header=None,dtype='uint8')\n",
    "data2=data2.astype(np.uint8)\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ndkLE55MdvL",
    "outputId": "da5076cb-f5ae-4ee7-e15d-e70fb203014a"
   },
   "outputs": [],
   "source": [
    "data3= pd.read_csv(r'infantile_blur.csv',header=None,dtype='uint8')\n",
    "data3=data3.astype(np.uint8)\n",
    "data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5my-S1Ay1r8a"
   },
   "outputs": [],
   "source": [
    "data=data1.append(data2)\n",
    "data=data.append(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vbb-GTmMZ1iU"
   },
   "outputs": [],
   "source": [
    "data.columns = data.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e4qejrYOUnly",
    "outputId": "a302d233-2664-46db-ca25-d32190c72c84"
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "NDN263tFQxwx"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10108/483045394.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\dataset\\infantile\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import pyarrow.csv\n",
    "import pyarrow.parquet\n",
    "\n",
    "data.to_parquet(r\"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\dataset\\infantile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "jSEP2mFu1r8a"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10108/2815861105.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcsv_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\dataset\\infantilecsv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "csv_data=data.to_csv(r\"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\dataset\\infantilecsv\", mode='a', header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDHt9b3u1r8c"
   },
   "source": [
    "# Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HmZeKQzzUlmt"
   },
   "outputs": [],
   "source": [
    "data = pd.read_parquet(r\"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\dataset\\infantile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "xeeXM7oUpv0g",
    "outputId": "c4b604fe-2745-4464-bfb7-bf02f8c4c029"
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OWDhYszrPIie"
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5)\n",
    "kmeans.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWjMkXsA1r8e"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nqzkw4eA1r8f"
   },
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = r'C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\dataset\\kmeans.sav'\n",
    "pickle.dump(kmeans, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "immtZ8WUUADN",
    "outputId": "b6e45231-9559-490f-b86f-763c77d22054"
   },
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ktyI-nmMmf_5"
   },
   "outputs": [],
   "source": [
    "kmeans = pickle.load(open(r'C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\dataset\\kmeans.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CHIZ7OYs1r8f",
    "outputId": "b0c714a8-cc1f-49fd-caa9-f5efa11f7f45"
   },
   "outputs": [],
   "source": [
    "hist=np.histogram(kmeans.labels_,bins=[0,1,2,3,4,5])\n",
    "\n",
    "\n",
    "print('histogram of trained kmeans')\n",
    "print(hist,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vhUd2cC81r8g",
    "outputId": "a9bdd3fc-e530-41e7-db76-858bc34b782c"
   },
   "outputs": [],
   "source": [
    "#performing kmeans prediction of the entire  dataset with the pretrained kmeans model\n",
    "\n",
    "#initialising i=0; as its the first class\n",
    "i=0\n",
    "data=[]\n",
    "#k=0\n",
    "folder1 = r'C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\folder1'\n",
    "for filename in os.listdir(folder1):\n",
    "    #path\n",
    "    path=os.path.join(folder1,filename)\n",
    "    a=cv2.imread(path)\n",
    "    \n",
    "    #resize image\n",
    "    resize=(280,430)\n",
    "    img=cv2.resize(a,resize)\n",
    "    \n",
    "    #gray image\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    \n",
    "    keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "    out=pd.DataFrame(descriptors)\n",
    "    \n",
    "    #predict values of feature vector with pretrained kmeans\n",
    "    #ValueError: Buffer dtype mismatch, expected 'float', in order to avoid this dtype=np.double\n",
    "    \n",
    "    array_double = np.array(out, dtype=np.double)\n",
    "    a=kmeans.predict(array_double)\n",
    "    \n",
    "    #hist= np.histogram(a,bins=[0,1,2,3,4,5,6,7,8])\n",
    "    hist= np.histogram(a,bins=[0,1,2,3,4,5])\n",
    "    #append the dataframe into the array in append mode, the array will only have 5 values which will store the values in a row\n",
    "    data.append(hist[0])\n",
    "    #k=k+1\n",
    "    \n",
    "#convert Array to Dataframe and append to the list\n",
    "Output = pd.DataFrame(data)\n",
    "#add row class \n",
    "Output[\"Class\"] = i \n",
    "csv_data=Output.to_csv('inf.csv', mode='a',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OwHvWE4xWz0m",
    "outputId": "e3c1b161-df7a-4e6d-9fe7-71311e086e1b"
   },
   "outputs": [],
   "source": [
    "np.histogram(a,bins=[0,1,2,3,4,5])[0] #test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mLmcqQ3L1r8h",
    "outputId": "8dcda450-45b2-4920-f95f-6e6517051c27"
   },
   "outputs": [],
   "source": [
    "#performing kmeans prediction of the entire bus dataset with the pretrained kmeans model\n",
    "\n",
    "#initialising i=1; as its the 2nd class\n",
    "i=1\n",
    "data=[]\n",
    "k=0\n",
    "folder2 = r'C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\folder2'\n",
    "for filename in os.listdir(folder2):\n",
    "      path=os.path.join(folder2,filename)\n",
    "      a=cv2.imread(path)\n",
    "      #resize image\n",
    "      resize=(280,430)\n",
    "      img=cv2.resize(a,resize)\n",
    "      \n",
    "      #gray image\n",
    "      gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "      \n",
    "      sift = cv2.xfeatures2d.SIFT_create()\n",
    "      \n",
    "      keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "      out=pd.DataFrame(descriptors)\n",
    "      \n",
    "      #predict values of feature vector with pretrained kmeans\n",
    "      #ValueError: Buffer dtype mismatch, expected 'float', in order to avoid this dtype=np.double\n",
    "      \n",
    "      array_double = np.array(out, dtype=np.double)\n",
    "      a=kmeans.predict(array_double)\n",
    "      \n",
    "      hist= np.histogram(a,bins=[0,1,2,3,4,5])\n",
    "      \n",
    "      #append the dataframe into the array in append mode, the array will only have 5 values which will store the values in a row\n",
    "      data.append(hist[0])\n",
    "      k=k+1\n",
    "    \n",
    "#convert Array to Dataframe and append to the list\n",
    "Output = pd.DataFrame(data)\n",
    "#add row class \n",
    "Output[\"Class\"] = i \n",
    "csv_data=Output.to_csv('bus_km_8_430.csv', mode='a',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tQZI_mcGe5JZ",
    "outputId": "56babddd-3d48-4889-a17d-4b43b7391e2b"
   },
   "outputs": [],
   "source": [
    "#performing kmeans prediction of the entire negative dataset with the pretrained kmeans model\n",
    "\n",
    "#initialising i=1; as its the 2nd class\n",
    "i=2\n",
    "data=[]\n",
    "k=0\n",
    "folder3 = r'C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\folder3'\n",
    "for filename in os.listdir(folder3):\n",
    "      path=os.path.join(folder3,filename)\n",
    "      a=cv2.imread(path)\n",
    "      #resize image\n",
    "      resize=(230,480)\n",
    "      img=cv2.resize(a,resize)\n",
    "      \n",
    "      #gray image\n",
    "      gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "      \n",
    "      sift = cv2.xfeatures2d.SIFT_create()\n",
    "      \n",
    "      keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "      out=pd.DataFrame(descriptors)\n",
    "      \n",
    "      #predict values of feature vector with pretrained kmeans\n",
    "      #ValueError: Buffer dtype mismatch, expected 'float', in order to avoid this dtype=np.double\n",
    "      \n",
    "      array_double = np.array(out, dtype=np.double)\n",
    "      a=kmeans.predict(array_double)\n",
    "      \n",
    "      hist= np.histogram(a,bins=[0,1,2,3,4,5])\n",
    "      \n",
    "      #append the dataframe into the array in append mode, the array will only have 5 values which will store the values in a row\n",
    "      data.append(hist[0])\n",
    "      k=k+1\n",
    "    \n",
    "#convert Array to Dataframe and append to the list\n",
    "Output = pd.DataFrame(data)\n",
    "#add row class \n",
    "Output[\"Class\"] = i \n",
    "csv_data=Output.to_csv('neg_km_5_430.csv', mode='a',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8gd-A1ReHOgM"
   },
   "outputs": [],
   "source": [
    "csv_data=Output.to_csv('FinalNstr.csv', mode='a',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6OYBQlro1r8h"
   },
   "outputs": [],
   "source": [
    "#Displaying the kmeans predicted data\n",
    "print(\"R\")\n",
    "dat1= pd.read_csv(r'C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\acne_infantile.csv',header=None)\n",
    "print(dat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNK7e2Oo1r8i"
   },
   "outputs": [],
   "source": [
    "print(\"B\")\n",
    "dat2= pd.read_csv(r'C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\acne_vulgaris.csv',header=None)\n",
    "print(dat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tBnDfmSmg67n"
   },
   "outputs": [],
   "source": [
    "print(\"N\")\n",
    "dat3= pd.read_csv(r'C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\infantile_blur.csv',header=None)\n",
    "print(dat3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2uW6rIK1r8i"
   },
   "outputs": [],
   "source": [
    "#appending All classes into 1 csv file\n",
    "\n",
    "A=dat1.append(dat2)\n",
    "A = A.append(dat3)\n",
    "csv_data=A.to_csv('FinalNstr.csv', mode='a',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Ko1kHbyGLFG"
   },
   "outputs": [],
   "source": [
    "A = pd.read_csv(\"FinalNstr.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QHmVghBZ45Q"
   },
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "cssdIz0j1r8j",
    "outputId": "31d5d27b-f07c-4ea1-b4da-7947c5eed125"
   },
   "outputs": [],
   "source": [
    "A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QK9XrUbg1r8j"
   },
   "outputs": [],
   "source": [
    "x = A.iloc[:,0:5].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KC5MRhOC1r8k"
   },
   "outputs": [],
   "source": [
    "y = A.iloc[:,5].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECHTTUIc1r8k"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "DhoniS = StandardScaler()\n",
    "Dhoni = DhoniS.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IxLC6yJW1r8k",
    "outputId": "04f26f62-f4a5-4eb9-f208-298d35942d44"
   },
   "outputs": [],
   "source": [
    "Dhoni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MNoIPzCF1r8k"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WujWbvtg1r8m",
    "outputId": "177d74f2-56dc-40a7-c72b-0544a8b5a612"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=5)\n",
    "pca.fit(Dhoni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bjZ4WWEh1r8n"
   },
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'PCA.sav'\n",
    "pickle.dump(pca, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y36jGfRm1r8n"
   },
   "outputs": [],
   "source": [
    "Bhumi = pca.transform(Dhoni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fxxlQ4l41r8n",
    "outputId": "f9411ee8-aaa5-4b62-e127-03cf29845e37"
   },
   "outputs": [],
   "source": [
    "Bhumi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nJL7lqnt1r8n",
    "outputId": "516510e1-2aa9-4fa3-b163-294125d4e7bb"
   },
   "outputs": [],
   "source": [
    "Bhumi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m-RTeShI1r8n",
    "outputId": "a0ca9540-23b9-4824-e1da-955aeb8e825c"
   },
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qktmjao71r8o"
   },
   "outputs": [],
   "source": [
    "Bhumi = pd.DataFrame(Bhumi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "FIMeE_M31r8o",
    "outputId": "5140736b-98c9-4b06-a911-40d0c2d4ee41"
   },
   "outputs": [],
   "source": [
    "Bhumi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3R5q-DHd1r8o"
   },
   "outputs": [],
   "source": [
    "B=pd.concat([Bhumi, pd.DataFrame(y)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X7qRiEh31r8o"
   },
   "outputs": [],
   "source": [
    "csv_data=B.to_csv('FinalPCAFV5_430_5.csv', mode='a',header=False,index=False) #SAVING PCA OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-yNivjzXYO_"
   },
   "source": [
    "# TESTING THE DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0b4vpZhIUGM"
   },
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_YrMHmrsH1Vl"
   },
   "outputs": [],
   "source": [
    "kmeans = pickle.load(open(r'C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\dataset\\kmeans.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OnKV-WJ8IKNM",
    "outputId": "e324350d-9464-4d6a-aa9f-9b0ad031f0e2"
   },
   "outputs": [],
   "source": [
    "hist=np.histogram(kmeans.labels_,bins=[0,1,2,3,4,5])\n",
    "\n",
    "\n",
    "print('histogram of trained kmeans')\n",
    "print(hist,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JrQ-yF6iIvpF"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QqHdzDEDaIPR"
   },
   "outputs": [],
   "source": [
    "scaler = pickle.load(open(r\"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\PCA.sav\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sn8B__AfJGoF"
   },
   "outputs": [],
   "source": [
    "pca = pickle.load(open(r\"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\PCA.sav\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y-dbLq0_JNzV",
    "outputId": "aa7f3bec-92dd-4b4f-af48-aad2053ecb31"
   },
   "outputs": [],
   "source": [
    "data= pd.read_csv(r\"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\FinalPCAFV5_430_5.csv\",header=None)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mkOgOPNrJXh9"
   },
   "outputs": [],
   "source": [
    "#assigning x the columns from 1 to 128 for training\n",
    "x = data.iloc[:,0:5].values\n",
    "#assigning y with the column \"Class\" as target variable\n",
    "y = data.iloc[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T02nC3P9JYxO"
   },
   "outputs": [],
   "source": [
    "#Dataset split into train and test with 80% Training and 20% Testing\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1oCREAyJvwd"
   },
   "source": [
    "# TEST DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "GBz8cAcxKYpG"
   },
   "outputs": [],
   "source": [
    "def predictor(path, model):\n",
    "  j = 0\n",
    "  gath = []\n",
    "  for i in os.listdir(path):\n",
    "    if j == 200:\n",
    "      break\n",
    "    img=cv2.imread(os.path.join(path,i))\n",
    "    resize=(280,430)\n",
    "\n",
    "    #resize image\n",
    "    #img=cv2.resize(a,resize)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #initialise sift descriptor\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "    out=pd.DataFrame(descriptors)\n",
    "    #initialise Kmeans and create 5 clusters\n",
    "    #test the model for the features i.e. for all elements in the Dataframe\n",
    "    array_double = np.array(out, dtype=np.double)\n",
    "    a=kmeans.predict(array_double)\n",
    "    hist=np.histogram(a,bins=[0,1,2,3,4,5])\n",
    "    #Output = pd.DataFrame([hist[0]])\n",
    "    gath.append(hist[0])\n",
    "    j+=1\n",
    "  scaler1 = pickle.load(open(r\"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\PCA.sav\", 'rb'))\n",
    "  pca = pickle.load(open(r\"C:\\Users\\tejal\\Desktop\\Tejal Salunke\\college\\CV\\CV project\\PCA.sav\", 'rb'))\n",
    "  x = scaler.fit_transform(pd.DataFrame(gath))\n",
    "  y = pca.transform(x)\n",
    "  y_pred1 = model.predict(y)\n",
    "  #prints the prediction of the class\n",
    "  return y_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uHi4c5CkMVIr",
    "outputId": "e2ec13eb-f9e0-4867-8313-99e9cf29f5b8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rfc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10108/3784684053.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfolder1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrfc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rfc' is not defined"
     ]
    }
   ],
   "source": [
    "path = folder1\n",
    "model = rfc\n",
    "predictor(path, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paNEJgb5ZBmm"
   },
   "source": [
    "# DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rm5kLSN0Jct9",
    "outputId": "1dbde441-55b3-46a0-bf18-90a631db034f"
   },
   "outputs": [],
   "source": [
    "#Decision Tree Classifier\n",
    "#Assign model with Decision Tree classifier\n",
    "model1 = DecisionTreeClassifier(max_depth=40)\n",
    "#training the model with the Training Variables \n",
    "model1.fit(x_train, y_train)\n",
    "#predicting the traget variable using testing variables\n",
    "y_pred1 = model1.predict(x_test)\n",
    "#Results\n",
    "print(\"Decision Tree Results\")\n",
    "print(\"Decision Tree Accuracy: \",accuracy_score(y_test, y_pred1)*100,\"%\")\n",
    "print(\"CM = \\n\",confusion_matrix(y_test, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lrzraMAKlWjJ"
   },
   "outputs": [],
   "source": [
    "para = [] # list to save all parameter values to plot\n",
    "acc = [] # list to save all accuracy values to plot\n",
    "for i in range(10,200,10):\n",
    "  model1 = DecisionTreeClassifier(max_depth=i)\n",
    "  model1.fit(x_train, y_train) #fitting the model\n",
    "  y_pred1 = model1.predict(x_test) # predicting the output\n",
    "  para.append(i) #saving to list\n",
    "  acc.append(accuracy_score(y_test, y_pred1)*100) #saving to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "geZu44_7mv_y",
    "outputId": "28395b41-00f4-4f5e-d25d-47301b45f1e5"
   },
   "outputs": [],
   "source": [
    "plt.plot(para,acc) #plotting accuracy vs parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoNgCeTKeYzY"
   },
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6kC_Nso-fLAq"
   },
   "outputs": [],
   "source": [
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PH7O0-WYfOk4",
    "outputId": "e6c10e18-5489-4bda-cf93-fe73235f483b"
   },
   "outputs": [],
   "source": [
    "print(\"OVA\")\n",
    "p = dict()\n",
    "for i in kernels:\n",
    "  SVM = SVC(kernel=i, decision_function_shape='ovr')\n",
    "  SV = SVM.fit(x_train, y_train)\n",
    "  y_pred1 = SV.predict(x_test)\n",
    "  #Results\n",
    "  p[i] = accuracy_score(y_test, y_pred1)*100\n",
    "  print(\"SVM \" + i +\" Results\")\n",
    "  print(\"SVM  \" + i +\" Accuracy: \",accuracy_score(y_test, y_pred1)*100,\"%\")\n",
    "  print(\"CM = \\n\",confusion_matrix(y_test, y_pred1))\n",
    "  print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "J3v40Vfqp8DB",
    "outputId": "50181862-d895-403d-b053-8c4ef1c30b61"
   },
   "outputs": [],
   "source": [
    "plt.bar(p.keys(),p.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CHLkVtBjgDSJ",
    "outputId": "0340fe79-8b88-43b7-8333-15de463a6abe"
   },
   "outputs": [],
   "source": [
    "p1 = dict()\n",
    "print(\"OVO\")\n",
    "for i in kernels:\n",
    "  SVM = SVC(kernel=i, decision_function_shape='ovo' )\n",
    "  SV = SVM.fit(x_train, y_train)\n",
    "  y_pred1 = SV.predict(x_test)\n",
    "  #Results\n",
    "  p1[i] = accuracy_score(y_test, y_pred1)*100\n",
    "  print(\"SVM \" + i +\" Results\")\n",
    "  print(\"SVM  \" + i +\" Accuracy: \",accuracy_score(y_test, y_pred1)*100,\"%\")\n",
    "  print(\"CM = \\n\",confusion_matrix(y_test, y_pred1))\n",
    "  print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "k2q3UQDkqXQB",
    "outputId": "ebd04624-3b4a-459e-e2c0-b7fd42b64256"
   },
   "outputs": [],
   "source": [
    "plt.bar(p1.keys(),p1.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2OThE9Vgavv"
   },
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wcZzCzMPo0OZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aws74W0xnKIY"
   },
   "outputs": [],
   "source": [
    "para = [] # list to save all parameter values to plot\n",
    "acc = [] # list to save all accuracy values to plot\n",
    "for i in range(10,500,10):\n",
    "  rfc = RandomForestClassifier(n_estimators=i)\n",
    "  rfc = rfc.fit(x_train,y_train)\n",
    "  y_pred1 = rfc.predict(x_test)\n",
    "  para.append(i)\n",
    "  acc.append(accuracy_score(y_test, y_pred1)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i0Wwj_cena_Y",
    "outputId": "2fe4e7e0-30db-41c3-dc07-99a23942ebf4"
   },
   "outputs": [],
   "source": [
    "plt.plot(para,acc) #plotting accuracy vs parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YahdsB0BoTkS",
    "outputId": "99f37b26-7cd0-4000-cbe2-59a06cac9f8f"
   },
   "outputs": [],
   "source": [
    "para[acc.index(max(acc))], max(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xSv5DMcfgeSJ",
    "outputId": "4b58d88d-7a15-49de-838a-a061aa7cde18"
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=350)\n",
    "rfc = rfc.fit(x_train,y_train)\n",
    "y_pred1 = rfc.predict(x_test)\n",
    "#Results\n",
    "print(\"RandomForestClassifier Results\")\n",
    "print(\"RandomForestClassifier Accuracy: \",accuracy_score(y_test, y_pred1)*100,\"%\")\n",
    "print(\"CM = \\n\",confusion_matrix(y_test, y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DnYNsNu7hkqb"
   },
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "w_K-nod0hmkZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9O3dZdnGo--h"
   },
   "outputs": [],
   "source": [
    "para = []\n",
    "acc = []\n",
    "for i in range(1,50,1):\n",
    "  knc = KNeighborsClassifier(n_neighbors= i,p = 3)\n",
    "  knc = knc.fit(x_train,y_train)\n",
    "  y_pred1 = knc.predict(x_test)\n",
    "  para.append(i)\n",
    "  acc.append(accuracy_score(y_test, y_pred1)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "hcksssnxpRLR",
    "outputId": "3ffa1306-b3f3-4d52-c025-45fc59fa364b"
   },
   "outputs": [],
   "source": [
    "plt.plot(para,acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XfoilTdgpVTh",
    "outputId": "f98c114d-3ffe-440a-e1c4-7f3cff60f8ee"
   },
   "outputs": [],
   "source": [
    "para[acc.index(max(acc))], max(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TnsO6zyrh3p4"
   },
   "outputs": [],
   "source": [
    "knc = KNeighborsClassifier(n_neighbors= 7,p = 3)\n",
    "knc = knc.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZlGgE1uKh_qJ",
    "outputId": "902f916b-5b9a-4d15-d403-d3a73e41ec96"
   },
   "outputs": [],
   "source": [
    "y_pred1 = knc.predict(x_test)\n",
    "#Results\n",
    "print(\"KNN Results\")\n",
    "print(\"KNN Accuracy: \",accuracy_score(y_test, y_pred1)*100,\"%\")\n",
    "print(\"CM = \\n\",confusion_matrix(y_test, y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NT_dF_EJi7_n"
   },
   "source": [
    "# GNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "R8q_KVTXiSj5"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8464/4288027418.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mgnb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mgnb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb = gnb.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0AzpHQyjt3o",
    "outputId": "113efc04-833b-4a01-cc6f-ed1215281c12"
   },
   "outputs": [],
   "source": [
    "y_pred1 = knc.predict(x_test)\n",
    "#Results\n",
    "print(\"GaussianNB Results\")\n",
    "print(\"GaussianNB Accuracy: \",accuracy_score(y_test, y_pred1)*100,\"%\")\n",
    "print(\"CM = \\n\",confusion_matrix(y_test, y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bw-cDhQ1WLz4"
   },
   "source": [
    "# TEST DT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRlKBdvLWDHJ"
   },
   "source": [
    "TESTING THE BEST MODEL (Random Forest) USING FOR LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43_jtGGTWLz4"
   },
   "outputs": [],
   "source": [
    "def predictor(path, model):\n",
    "  j = 0\n",
    "  gath = []\n",
    "  for i in os.listdir(path):\n",
    "    if j == 200:\n",
    "      break\n",
    "    img=cv2.imread(os.path.join(path,i))\n",
    "    resize=(280,430)\n",
    "\n",
    "    #resize image\n",
    "    #img=cv2.resize(a,resize)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #initialise sift descriptor\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "    out=pd.DataFrame(descriptors)\n",
    "    #initialise Kmeans and create 5 clusters\n",
    "    #test the model for the features i.e. for all elements in the Dataframe\n",
    "    array_double = np.array(out, dtype=np.double)\n",
    "    a=kmeans.predict(array_double)\n",
    "    hist=np.histogram(a,bins=[0,1,2,3,4,5])\n",
    "    #Output = pd.DataFrame([hist[0]])\n",
    "    gath.append(hist[0])\n",
    "    j+=1\n",
    "  scaler1 = pickle.load(open(\"/content/drive/MyDrive/CV_CP/430_all/PCA_5_430_SCALER.sav\", 'rb'))\n",
    "  pca = pickle.load(open(\"/content/drive/MyDrive/CV_CP/430_all/PCA_5_430_Model.sav\", 'rb'))\n",
    "  x = scaler.fit_transform(pd.DataFrame(gath))\n",
    "  y = pca.transform(x)\n",
    "  y_pred1 = model.predict(y)\n",
    "  #prints the prediction of the class\n",
    "  return y_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BGEHxjtEWLz5",
    "outputId": "e2ec13eb-f9e0-4867-8313-99e9cf29f5b8"
   },
   "outputs": [],
   "source": [
    "path = folder1\n",
    "model = rfc\n",
    "predictor(path, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-utXwgeZBcCs"
   },
   "source": [
    "# Dropping Negative Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "egWhn8XIBg1k"
   },
   "outputs": [],
   "source": [
    "data2 = data[data[5] != 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xBBMx6GrCM9h"
   },
   "outputs": [],
   "source": [
    "#assigning x the columns from 1 to 128 for training\n",
    "x = data2.iloc[:,0:5].values\n",
    "#assigning y with the column \"Class\" as target variable\n",
    "y = data2.iloc[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nxo9yzF0CM9j"
   },
   "outputs": [],
   "source": [
    "#Dataset split into train and test with 80% Training and 20% Testing\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fWsEoDKCVbn"
   },
   "source": [
    "# DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lwiq3fbNCVbo",
    "outputId": "b3ec9d54-393a-4aeb-bbcf-e4dd506693cf"
   },
   "outputs": [],
   "source": [
    "#Decision Tree Classifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "#Assign model with Decision Tree classifier\n",
    "model1 = DecisionTreeClassifier(max_depth=40)\n",
    "#training the model with the Training Variables \n",
    "model1.fit(x_train, y_train)\n",
    "#predicting the traget variable using testing variables\n",
    "y_pred1 = model1.predict(x_test)\n",
    "#Results\n",
    "print(\"Decision Tree Results\")\n",
    "print(\"Decision Tree Accuracy: \",accuracy_score(y_test, y_pred1)*100,\"%\")\n",
    "print(\"CM = \\n\",confusion_matrix(y_test, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPHKEb5HCVbp"
   },
   "outputs": [],
   "source": [
    "para = [] # list to save all parameter values to plot\n",
    "acc = [] # list to save all accuracy values to plot\n",
    "for i in range(10,200,10):\n",
    "  model1 = DecisionTreeClassifier(max_depth=i)\n",
    "  model1.fit(x_train, y_train) #fitting the model\n",
    "  y_pred1 = model1.predict(x_test) # predicting the output\n",
    "  para.append(i) #saving to list\n",
    "  acc.append(accuracy_score(y_test, y_pred1)*100) #saving to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FqdZZ2h7CVbp",
    "outputId": "65c3e276-3d1a-41af-c8a2-af96868b71b7"
   },
   "outputs": [],
   "source": [
    "plt.plot(para,acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXtQCvWiCVbp"
   },
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8wzQ9B6gCVbq"
   },
   "outputs": [],
   "source": [
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid'] #making a list of kernals for easy looping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "brIdDrpyCoQc"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC #importing the required model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s3kjr4nnCVbq",
    "outputId": "d81be45f-f947-49f2-ab0d-539ef7826a04"
   },
   "outputs": [],
   "source": [
    "print(\"OVA\")\n",
    "p = dict()\n",
    "for i in kernels:\n",
    "  SVM = SVC(kernel=i, decision_function_shape='ovr')\n",
    "  SV = SVM.fit(x_train, y_train)\n",
    "  y_pred1 = SV.predict(x_test)\n",
    "  #Results\n",
    "  p[i] = accuracy_score(y_test, y_pred1)*100\n",
    "  print(\"SVM \" + i +\" Results\")\n",
    "  print(\"SVM  \" + i +\" Accuracy: \",accuracy_score(y_test, y_pred1)*100,\"%\")\n",
    "  print(\"CM = \\n\",confusion_matrix(y_test, y_pred1))\n",
    "  print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6z41kEhCCVbr",
    "outputId": "7f759e27-2ff4-48dc-e06f-0fcd09e6759c"
   },
   "outputs": [],
   "source": [
    "plt.bar(p.keys(),p.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2UM-c4V8CVbr",
    "outputId": "ef4171d6-f236-45c4-fc19-2e46521d38c6"
   },
   "outputs": [],
   "source": [
    "p1 = dict()\n",
    "print(\"OVO\")\n",
    "for i in kernels:\n",
    "  SVM = SVC(kernel=i, decision_function_shape='ovo' )\n",
    "  SV = SVM.fit(x_train, y_train)\n",
    "  y_pred1 = SV.predict(x_test)\n",
    "  #Results\n",
    "  p1[i] = accuracy_score(y_test, y_pred1)*100\n",
    "  print(\"SVM \" + i +\" Results\")\n",
    "  print(\"SVM  \" + i +\" Accuracy: \",accuracy_score(y_test, y_pred1)*100,\"%\")\n",
    "  print(\"CM = \\n\",confusion_matrix(y_test, y_pred1))\n",
    "  print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JN6wBUPWCVbr",
    "outputId": "ef01508f-7a44-4c95-bb55-abc5b3299224"
   },
   "outputs": [],
   "source": [
    "plt.bar(p1.keys(),p1.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQn6DbZqCVbr"
   },
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "iwcH98R5CVbr"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1zMzgPDCVbr"
   },
   "outputs": [],
   "source": [
    "para = []\n",
    "acc = []\n",
    "for i in range(10,500,10):\n",
    "  rfc = RandomForestClassifier(n_estimators=i)\n",
    "  rfc = rfc.fit(x_train,y_train)\n",
    "  y_pred1 = rfc.predict(x_test)\n",
    "  para.append(i)\n",
    "  acc.append(accuracy_score(y_test, y_pred1)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-0lBDB0ECVbs",
    "outputId": "79282fba-0877-4222-d504-b36a8a8b4306"
   },
   "outputs": [],
   "source": [
    "plt.plot(para,acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1xT4PNDXCVbs",
    "outputId": "d08defbf-2920-452f-bf2b-a57073269877"
   },
   "outputs": [],
   "source": [
    "para[acc.index(max(acc))], max(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Lyn_2fGCVbs",
    "outputId": "6706aead-d860-403f-9142-fae13e71a906"
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=420)\n",
    "rfc = rfc.fit(x_train,y_train)\n",
    "y_pred1 = rfc.predict(x_test)\n",
    "#Results\n",
    "print(\"RandomForestClassifier Results\")\n",
    "print(\"RandomForestClassifier Accuracy: \",accuracy_score(y_test, y_pred1)*100,\"%\")\n",
    "print(\"CM = \\n\",confusion_matrix(y_test, y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTDSSax8CVbs"
   },
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "M7N3NUCzCVbs"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2vmWSnSxCVbs"
   },
   "outputs": [],
   "source": [
    "para = []\n",
    "acc = []\n",
    "for i in range(1,50,1):\n",
    "  knc = KNeighborsClassifier(n_neighbors= i,p = 3)\n",
    "  knc = knc.fit(x_train,y_train)\n",
    "  y_pred1 = knc.predict(x_test)\n",
    "  para.append(i)\n",
    "  acc.append(accuracy_score(y_test, y_pred1)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YFlu1LpECVbs",
    "outputId": "28043cbc-ff65-4fba-c1f4-141ffb633529"
   },
   "outputs": [],
   "source": [
    "plt.plot(para,acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pP25svMUCVbs",
    "outputId": "96f435a0-24e1-47ca-cf06-e777311d799f"
   },
   "outputs": [],
   "source": [
    "para[acc.index(max(acc))], max(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YKWTM0m9CVbt"
   },
   "outputs": [],
   "source": [
    "knc = KNeighborsClassifier(n_neighbors= 7,p = 3)\n",
    "knc = knc.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n_DNDlj5CVbt",
    "outputId": "fad94d09-e270-4900-8eb5-cc65859a134e"
   },
   "outputs": [],
   "source": [
    "y_pred1 = knc.predict(x_test)\n",
    "#Results\n",
    "print(\"KNN Results\")\n",
    "print(\"KNN Accuracy: \",accuracy_score(y_test, y_pred1)*100,\"%\")\n",
    "print(\"CM = \\n\",confusion_matrix(y_test, y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jCTdh_YCVbt"
   },
   "source": [
    "# GNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "um0DOKIXCVbt"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8464/4288027418.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mgnb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mgnb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb = gnb.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q_YAZYYECVbt",
    "outputId": "7bc18ed5-9117-4005-e2b2-dfcee181f201"
   },
   "outputs": [],
   "source": [
    "y_pred1 = gnb.predict(x_test)\n",
    "#Results\n",
    "print(\"GaussianNB Results\")\n",
    "print(\"GaussianNB Accuracy: \",accuracy_score(y_test, y_pred1)*100,\"%\")\n",
    "print(\"CM = \\n\",confusion_matrix(y_test, y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Ym7n2hPY9V6"
   },
   "source": [
    "# TEST DT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-bTtusPY9V7"
   },
   "source": [
    "TESTING THE BEST MODEL (Random Forest) USING FOR LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K86STwBcY9V7"
   },
   "outputs": [],
   "source": [
    "def predictor(path, model):\n",
    "  j = 0\n",
    "  gath = []\n",
    "  for i in os.listdir(path):\n",
    "    if j == 200:\n",
    "      break\n",
    "    img=cv2.imread(os.path.join(path,i))\n",
    "    resize=(280,430)\n",
    "\n",
    "    #resize image\n",
    "    #img=cv2.resize(a,resize)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #initialise sift descriptor\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "    out=pd.DataFrame(descriptors)\n",
    "    #initialise Kmeans and create 5 clusters\n",
    "    #test the model for the features i.e. for all elements in the Dataframe\n",
    "    array_double = np.array(out, dtype=np.double)\n",
    "    a=kmeans.predict(array_double)\n",
    "    hist=np.histogram(a,bins=[0,1,2,3,4,5])\n",
    "    #Output = pd.DataFrame([hist[0]])\n",
    "    gath.append(hist[0])\n",
    "    j+=1\n",
    "  scaler1 = pickle.load(open(\"/content/drive/MyDrive/CV_CP/430_all/PCA_5_430_SCALER.sav\", 'rb'))\n",
    "  pca = pickle.load(open(\"/content/drive/MyDrive/CV_CP/430_all/PCA_5_430_Model.sav\", 'rb'))\n",
    "  x = scaler.fit_transform(pd.DataFrame(gath))\n",
    "  y = pca.transform(x)\n",
    "  y_pred1 = model.predict(y)\n",
    "  #prints the prediction of the class\n",
    "  return y_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "06lJqNKEY9V8",
    "outputId": "8f71874c-beb0-48c5-9596-202e08f01b05"
   },
   "outputs": [],
   "source": [
    "path = folder1\n",
    "model = rfc\n",
    "predictor(path, model)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CV_CP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
